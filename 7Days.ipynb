{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a2d1f2",
   "metadata": {},
   "source": [
    "AIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85d05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tanker rows from files (only tanker rows will be loaded)...\n",
      "Unique tankers in data.csv: 463\n",
      "Unique tankers in data2.csv: 446\n",
      "Unique tankers in data3.csv: 441\n",
      "Unique tankers in data4.csv: 449\n",
      "Unique tankers in data5.csv: 456\n",
      "Unique tankers in data6.csv: 474\n",
      "Unique tankers in data7.csv: 397\n",
      "Present in all files: 241\n",
      "Unique tankers in data.csv: 463\n",
      "Unique tankers in data2.csv: 446\n",
      "Unique tankers in data3.csv: 441\n",
      "Unique tankers in data4.csv: 449\n",
      "Unique tankers in data5.csv: 456\n",
      "Unique tankers in data6.csv: 474\n",
      "Unique tankers in data7.csv: 397\n",
      "Present in all files: 241\n"
     ]
    }
   ],
   "source": [
    "# Stream-read data1..data7, extract tankers (VesselType==80), compute overlaps and produce a map\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium import IFrame, DivIcon, PolyLine\n",
    "from folium.plugins import Fullscreen\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize, rgb2hex, LinearSegmentedColormap\n",
    "\n",
    "# Helper: find a timestamp column in columns list\n",
    "def find_timestamp_col_from_cols(cols):\n",
    "    candidates = ['BaseDateTime','Timestamp','Date','Datetime','date','time','BaseDateTimeUTC','DateTimeUTC','received_at','timestamp']\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Candidate length column names\n",
    "length_candidates = ['Length','LengthOverall','LOA','LENGTH','length','Length(m)']\n",
    "\n",
    "# Small helper to stream-read only rows where VesselType == 80\n",
    "def read_tankers(path, chunksize=100_000):\n",
    "    desired = ['MMSI','BaseDateTime','LAT','LON','SOG','COG','Heading','VesselName','IMO','CallSign','VesselType','Status','Length','Width','Draft','Cargo','TransceiverClass']\n",
    "    hdr = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "    ts_col = find_timestamp_col_from_cols(hdr)\n",
    "    usecols = [c for c in desired if c in hdr]\n",
    "    for c in length_candidates:\n",
    "        if c in hdr and c not in usecols:\n",
    "            usecols.append(c)\n",
    "    if ts_col and ts_col not in usecols:\n",
    "        usecols.append(ts_col)\n",
    "    pieces = []\n",
    "    for chunk in pd.read_csv(path, usecols=usecols, chunksize=chunksize, low_memory=False):\n",
    "        if 'VesselType' in chunk.columns:\n",
    "            filtered = chunk[chunk['VesselType'] == 80]\n",
    "        else:\n",
    "            filtered = pd.DataFrame(columns=chunk.columns)\n",
    "        if not filtered.empty:\n",
    "            pieces.append(filtered)\n",
    "    if pieces:\n",
    "        df = pd.concat(pieces, ignore_index=True, sort=False)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=usecols)\n",
    "    return df, ts_col\n",
    "\n",
    "# Files in chronological order (data.csv oldest ... data7.csv newest)\n",
    "files = ['data.csv','data2.csv','data3.csv','data4.csv','data5.csv','data6.csv','data7.csv']\n",
    "\n",
    "# If the cell is re-run, avoid reloading large CSVs: use a simple in-memory cache flag\n",
    "if globals().get('_tankers_cache_loaded', False):\n",
    "    print('Tanker rows already loaded in memory — using cached results.')\n",
    "    try:\n",
    "        for f, s in zip(files, u_sets):\n",
    "            print(f'Unique tankers in {f}: {len(s)}')\n",
    "        common_all = set.intersection(*[s for s in u_sets if s]) if any(u_sets) else set()\n",
    "        print(f'Present in all files: {len(common_all)}')\n",
    "    except NameError:\n",
    "        print('Cached variables partially missing — will reload from files')\n",
    "        _tankers_cache_loaded = False\n",
    "\n",
    "# Read tanker rows from each file (streaming) if not already loaded\n",
    "if not globals().get('_tankers_cache_loaded', False):\n",
    "    print('Reading tanker rows from files (only tanker rows will be loaded)...')\n",
    "    dfs = []\n",
    "    ts_cols = []\n",
    "    u_sets = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df, ts = read_tankers(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found: {f} — skipping')\n",
    "            df = pd.DataFrame()\n",
    "            ts = None\n",
    "        if not df.empty:\n",
    "            df['__source'] = f\n",
    "        dfs.append(df)\n",
    "        ts_cols.append(ts)\n",
    "        if 'MMSI' in df.columns:\n",
    "            u_sets.append(set(df['MMSI'].dropna().unique()))\n",
    "        else:\n",
    "            u_sets.append(set())\n",
    "\n",
    "    # Print unique counts per file\n",
    "    for f, s in zip(files, u_sets):\n",
    "        print(f'Unique tankers in {f}: {len(s)}')\n",
    "\n",
    "    # Print some intersection stats\n",
    "    all_u = [s for s in u_sets]\n",
    "    if all_u:\n",
    "        common_all = set.intersection(*[s for s in all_u if s]) if any(all_u) else set()\n",
    "        print(f'Present in all files: {len(common_all)}')\n",
    "\n",
    "    # unify per-file ts column names\n",
    "    for df, ts in zip(dfs, ts_cols):\n",
    "        if ts and ts in df.columns:\n",
    "            df['_ts'] = pd.to_datetime(df[ts], errors='coerce')\n",
    "        else:\n",
    "            df['_ts'] = pd.NaT\n",
    "\n",
    "    # Combine datasets\n",
    "    t_all = pd.concat(dfs, ignore_index=True, sort=False) if dfs else pd.DataFrame()\n",
    "\n",
    "    # Build list of all MMSIs\n",
    "    all_mmsis = sorted(set(t_all['MMSI'].dropna().unique())) if 'MMSI' in t_all.columns else []\n",
    "\n",
    "    # Determine last position per MMSI preferring newest file (data7 highest priority)\n",
    "    last_positions = []\n",
    "    for mmsi in all_mmsis:\n",
    "        row = None\n",
    "        # iterate files from newest to oldest\n",
    "        for df, f in zip(dfs[::-1], files[::-1]):\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            grp = df[df['MMSI'] == mmsi]\n",
    "            # choose timestamp column for that df if available\n",
    "            ts = find_timestamp_col_from_cols(df.columns)\n",
    "            if ts and ts in grp.columns:\n",
    "                grp = grp.sort_values(ts)\n",
    "            if len(grp):\n",
    "                row = grp.iloc[-1]\n",
    "                break\n",
    "        if row is not None:\n",
    "            last_positions.append(row)\n",
    "    last_df = pd.DataFrame(last_positions) if last_positions else pd.DataFrame(columns=t_all.columns)\n",
    "\n",
    "    # Overall latest timestamp for disappearance detection\n",
    "    overall_latest = None\n",
    "    if '_ts' in t_all.columns and t_all['_ts'].notna().any():\n",
    "        overall_latest = t_all['_ts'].max()\n",
    "\n",
    "    # Compute length statistics from available rows\n",
    "    lengths = []\n",
    "    for _, r in t_all.iterrows():\n",
    "        L = None\n",
    "        for c in length_candidates:\n",
    "            if c in t_all.columns and pd.notna(r.get(c)):\n",
    "                try:\n",
    "                    L = float(r.get(c))\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if L is not None:\n",
    "            lengths.append(L)\n",
    "    if lengths:\n",
    "        min_len = float(np.nanmin(lengths))\n",
    "        max_len = float(np.nanmax(lengths))\n",
    "    else:\n",
    "        min_len, max_len = 0.0, 1.0\n",
    "    norm = Normalize(vmin=min_len, vmax=max_len)\n",
    "    # custom colormap: blue -> purple -> red\n",
    "    cmap = LinearSegmentedColormap.from_list('blue_purple_red', ['blue', 'purple', 'red'])\n",
    "\n",
    "    def color_for_length(L):\n",
    "        if L is None:\n",
    "            return '#800080'  # purple default for unknown\n",
    "        rgb = cmap(norm(L))[:3]\n",
    "        return rgb2hex(rgb)\n",
    "\n",
    "    # Helper to get length from a row\n",
    "    def get_length_from_row(r):\n",
    "        for c in length_candidates:\n",
    "            if c in r and pd.notna(r.get(c)):\n",
    "                try:\n",
    "                    return float(r.get(c))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    # Map center\n",
    "    if len(last_df):\n",
    "        center = [last_df['LAT'].astype(float).mean(), last_df['LON'].astype(float).mean()]\n",
    "    else:\n",
    "        center = [0, 0]\n",
    "\n",
    "    # Create map\n",
    "    m = folium.Map(location=center, zoom_start=5, tiles='CartoDB dark_matter')\n",
    "    folium.TileLayer('https://tiles.openseamap.org/seamark/{z}/{x}/{y}.png', name='SeaMarks', attr='OpenSeaMap - seamark').add_to(m)\n",
    "    Fullscreen().add_to(m)\n",
    "\n",
    "    # Plot trails (dashed) using timestamps where available\n",
    "    for mmsi in all_mmsis:\n",
    "        group = t_all[t_all['MMSI'] == mmsi].copy()\n",
    "        if '_ts' in group.columns and group['_ts'].notna().any():\n",
    "            group = group.sort_values('_ts')\n",
    "        coords = [(float(r['LAT']), float(r['LON'])) for _, r in group.iterrows() if pd.notna(r.get('LAT')) and pd.notna(r.get('LON'))]\n",
    "        if len(coords) >= 2:\n",
    "            trail_color = color_for_length(get_length_from_row(group.iloc[-1]))\n",
    "            PolyLine(locations=coords, color=trail_color, weight=2, opacity=0.85, dash_array='6,6').add_to(m)\n",
    "\n",
    "    # Add last-position markers with arrow size based on ship length\n",
    "    for _, row in last_df.iterrows():\n",
    "        lat = row.get('LAT')\n",
    "        lon = row.get('LON')\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            continue\n",
    "        mmsi = row.get('MMSI')\n",
    "        heading = row.get('Heading')\n",
    "        try:\n",
    "            heading = float(heading)\n",
    "        except Exception:\n",
    "            heading = 0.0\n",
    "        ship_length = get_length_from_row(row)\n",
    "        # map length to size: emphasize relative differences but reduce overall sizes\n",
    "        if ship_length is None or max_len == min_len:\n",
    "            svg_size = 10\n",
    "        else:\n",
    "            frac = (ship_length - min_len) / max(1e-6, (max_len - min_len))\n",
    "            raw = 10 + (frac ** 1.7) * 60\n",
    "            svg_size = max(8, int(raw * 0.5))\n",
    "        # disappearance detection\n",
    "        last_ts = row.get('_ts') if pd.notna(row.get('_ts')) else None\n",
    "        disappeared = False\n",
    "        if overall_latest is not None and last_ts is not None and pd.notna(last_ts):\n",
    "            try:\n",
    "                disappeared = (overall_latest - pd.to_datetime(last_ts)) > pd.Timedelta(days=1)\n",
    "            except Exception:\n",
    "                disappeared = False\n",
    "        ex_html = '<span style=\"position:absolute;top:0;right:0;color:#ff3333;font-weight:bold;font-size:16px;\">&#x2757;</span>' if disappeared else ''\n",
    "        color = color_for_length(ship_length)\n",
    "        # smaller arrows overall; size differences more noticeable\n",
    "        svg = f\"\"\"<div style=\\\"position:relative;width:{svg_size}px;height:{svg_size}px;display:flex;align-items:center;justify-content:center;\\\">{ex_html}\n",
    "    <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"{svg_size}\\\" height=\\\"{svg_size}\\\" viewBox=\\\"0 0 24 24\\\" style=\\\"transform: rotate({heading}deg); transform-origin: 12px 12px;\\\">\n",
    "          <path d=\\\"M12 2 L19 21 L12 17 L5 21 Z\\\" fill=\\\"{color}\\\" stroke=\\\"#000000\\\" stroke-width=\\\"0.6\\\" />\n",
    "        </svg>\n",
    "        </div>\"\"\"\n",
    "        icon = DivIcon(html=svg)\n",
    "        source = row.get('__source', '')\n",
    "        ts_val = row.get('_ts')\n",
    "        popup_html = f\"MMSI: {mmsi}<br>Vessel: {row.get('VesselName','')}<br>Length: {ship_length if ship_length is not None else 'N/A'} m<br>Speed (SOG): {row.get('SOG','')}<br>Heading: {heading}<br>Source: {source}<br>Time: {ts_val}\"\n",
    "        iframe = IFrame(popup_html, width=340, height=140)\n",
    "        popup = folium.Popup(iframe, max_width=380)\n",
    "        folium.Marker(location=[float(lat), float(lon)], icon=icon, popup=popup).add_to(m)\n",
    "\n",
    "    # Finish and save\n",
    "    folium.LayerControl().add_to(m)\n",
    "    m.save('tankers_last_positions_map.html')\n",
    "\n",
    "    # mark cache loaded so future runs are faster and won't reprint full read messages\n",
    "    _tankers_cache_loaded = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9ff0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
